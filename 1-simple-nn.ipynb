{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7c03c6-af65-493f-9768-b78bc87f5782",
   "metadata": {},
   "source": [
    "# Simple test for a simple NN with simple data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83a11b-f592-40e4-9f63-918f8d35efe1",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56b372-d48d-4e29-b851-115ff0f76c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003cc2e6-8e55-46ec-89e6-8a5a880d542c",
   "metadata": {},
   "source": [
    "Import libraries and create classes that we need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4a70d4-2eb6-447c-a3fb-9cc6889df0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "\n",
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return (correct / total_examples).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf3fca-cda5-4627-8a44-1686a2c4a1ef",
   "metadata": {},
   "source": [
    "Create data. For now they are gebnerated via code, but in reality these would be downloaded online and saved in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaf307ff-9ae7-427d-8aef-a1f108cc4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "\n",
    "y_test = torch.tensor([0, 1])\n",
    "\n",
    "train_ds = ToyDataset(X_train, y_train)\n",
    "test_ds = ToyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "test_ds = ToyDataset(X_test, y_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f54220-a83f-423c-895f-bc3fbbe9c59b",
   "metadata": {},
   "source": [
    "Now train a NN model using **cpu**. We set a seed so we can always expect the same results (good for debugging). We also print some accuracy metrics for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b57efd-5c5a-45a7-aea6-f0903f2da983",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CPU training\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork(num_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# CPU training\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels) # Loss function\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "# Optional model evaluation\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1)\n",
    "print(probas)\n",
    "\n",
    "compute_accuracy(model, train_loader)\n",
    "compute_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a4333-07d9-45b4-bbfd-70336e5469a8",
   "metadata": {},
   "source": [
    "**Saving and loading models**\n",
    "\n",
    "Avoid this cell for tests, but in the future we might need to store the trained model somewhere on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "879a0a84-7081-4f2d-b197-565599b8df55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not needed but sometimes we want to save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "\n",
    "model = NeuralNetwork(2, 2) # needs to match the original model exactly\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0cef3-5cb9-4b71-9a0a-97d4956e6cb6",
   "metadata": {},
   "source": [
    "Now train an NN model using **gpu**. We use the *.to()* method to transfer models and data onto a GPU and perform the computation there. If your machine hosts multiple GPUs, you have the option to specify which GPU you’d like to transfer the tensors to. You can do this by indicating the device ID in the transfer command. For instance, you can use *.to(\"cuda:0\")*, *.to(\"cuda:1\")*, and so on.\n",
    "\n",
    "We set a seed so we can always expect the same results (good for debugging). We also print some accuracy metrics for training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98565f64-2274-470c-8f79-60a6b51eef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single GPU training\n",
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "\n",
    "# New: Define a device variable that defaults to a GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "# New: Transfer the model onto the GPU.\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        # New: Transfer the data onto the GPU.\n",
    "        features, labels = features.to(device), labels.to(device)    #C\n",
    "        logits = model(features)\n",
    "        loss = F.cross_entropy(logits, labels) # Loss function\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "              f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "    # Optional model evaluation\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "probas = torch.softmax(outputs, dim=1)\n",
    "print(probas)\n",
    "\n",
    "compute_accuracy(model, train_loader)\n",
    "compute_accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3f318-be1a-4ede-96ac-51444fa870ab",
   "metadata": {},
   "source": [
    "In the next cell we use **PyTorch’s DistributedDataParallel (DDP)** strategy to train the model over multiple GPUs. DDP enables parallelism by splitting the input data across the available devices and processing these data subsets simultaneously. PyTorch launches a separate process on each GPU, and each process receives and keeps a copy of the model – these copies will be synchronized during training. Each of the two GPUs receives a copy of the model. Then, in every training iteration, each model receives a minibatch (or just batch) from the data loader. We can use a DistributedSampler to ensure that each GPU will receive a different, non-overlapping batch when using DDP.\n",
    "\n",
    "**Note:** DDP does not function properly within interactive Python environments like Jupyter notebooks, which don’t handle multiprocessing in the same way a standalone Python script does. Therefore, the following code should be executed as a script, not within a notebook interface like Jupyter. This is because DDP needs to spawn multiple processes, and each process should have its own Python interpreter instance.\n",
    "\n",
    "Also, rather than running the code as a “regular” Python script (via python ...py) and manually spawning processes from within Python using *multiprocessing.spawn* for the multi-GPU training aspect, we rely on PyTorch’s modern and preferred utility: *torchrun*. It automatically launches one process per GPU and assigns each process a unique rank, along with other distributed training metadata (like world size and local rank), which are passed into the script via environment variables. In the __main__ block, we read these variables using *os.environ* and pass them to the *main()* function.\n",
    "\n",
    "The *ddp_setup()* function sets the master node’s address and communication port (unless already provided by torchrun), initializes the process group using the NCCL backend (which is optimized for GPU-to-GPU communication), and then sets the device for the current process using the provided rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bfa2e-b5be-4bce-ab7e-c614642c3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-GPU training, full script\n",
    "# This script is designed for 2 GPUs. \n",
    "# After saving it as a file, DDP-script-torchrun.py, \n",
    "# you can run it as follows using the torchrun utility from the command line, \n",
    "# which is automatically installed when you install PyTorch, assuming you saved the above code as DDP-script-torchrun.py file\n",
    "\n",
    "# for 2 GPUs\n",
    "# torchrun --nproc_per_node=2 DDP-script-torchrun.py\n",
    "\n",
    "# If you want to run it on all available GPUs, you can use:\n",
    "#torchrun --nproc_per_node=$(nvidia-smi -L | wc -l) DDP-script-torchrun.py\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# NEW imports:\n",
    "import os\n",
    "import platform\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "\n",
    "# NEW: function to initialize a distributed process group (1 process / GPU)\n",
    "# this allows communication among processes\n",
    "def ddp_setup(rank, world_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        rank: a unique process ID\n",
    "        world_size: total number of processes in the group\n",
    "    \"\"\"\n",
    "    # Only set MASTER_ADDR and MASTER_PORT if not already defined by torchrun\n",
    "    if \"MASTER_ADDR\" not in os.environ:\n",
    "        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    if \"MASTER_PORT\" not in os.environ:\n",
    "        os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "\n",
    "    # initialize process group\n",
    "    if platform.system() == \"Windows\":\n",
    "        # Disable libuv because PyTorch for Windows isn't built with support\n",
    "        os.environ[\"USE_LIBUV\"] = \"0\"\n",
    "        # Windows users may have to use \"gloo\" instead of \"nccl\" as backend\n",
    "        # gloo: Facebook Collective Communication Library\n",
    "        init_process_group(backend=\"gloo\", rank=rank, world_size=world_size)\n",
    "    else:\n",
    "        # nccl: NVIDIA Collective Communication Library\n",
    "        init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x, one_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    X_train = torch.tensor([\n",
    "        [-1.2, 3.1],\n",
    "        [-0.9, 2.9],\n",
    "        [-0.5, 2.6],\n",
    "        [2.3, -1.1],\n",
    "        [2.7, -1.5]\n",
    "    ])\n",
    "    y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "    X_test = torch.tensor([\n",
    "        [-0.8, 2.8],\n",
    "        [2.6, -1.6],\n",
    "    ])\n",
    "    y_test = torch.tensor([0, 1])\n",
    "\n",
    "    # Uncomment these lines to increase the dataset size to run this script on up to 8 GPUs:\n",
    "    # factor = 4\n",
    "    # X_train = torch.cat([X_train + torch.randn_like(X_train) * 0.1 for _ in range(factor)])\n",
    "    # y_train = y_train.repeat(factor)\n",
    "    # X_test = torch.cat([X_test + torch.randn_like(X_test) * 0.1 for _ in range(factor)])\n",
    "    # y_test = y_test.repeat(factor)\n",
    "\n",
    "    train_ds = ToyDataset(X_train, y_train)\n",
    "    test_ds = ToyDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_ds,\n",
    "        batch_size=2,\n",
    "        shuffle=False,  # NEW: False because of DistributedSampler below\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        # NEW: chunk batches across GPUs without overlapping samples:\n",
    "        sampler=DistributedSampler(train_ds)  # NEW\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_ds,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# NEW: wrapper\n",
    "def main(rank, world_size, num_epochs):\n",
    "\n",
    "    ddp_setup(rank, world_size)  # NEW: initialize process groups\n",
    "\n",
    "    train_loader, test_loader = prepare_dataset()\n",
    "    model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
    "    model.to(rank)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "\n",
    "    model = DDP(model, device_ids=[rank])  # NEW: wrap model with DDP\n",
    "    # the core model is now accessible as model.module\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # NEW: Set sampler to ensure each epoch has a different shuffle order\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        for features, labels in train_loader:\n",
    "\n",
    "            features, labels = features.to(rank), labels.to(rank)  # New: use rank\n",
    "            logits = model(features)\n",
    "            loss = F.cross_entropy(logits, labels)  # Loss function\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # LOGGING\n",
    "            print(f\"[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "                  f\" | Batchsize {labels.shape[0]:03d}\"\n",
    "                  f\" | Train/Val Loss: {loss:.2f}\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        train_acc = compute_accuracy(model, train_loader, device=rank)\n",
    "        print(f\"[GPU{rank}] Training accuracy\", train_acc)\n",
    "        test_acc = compute_accuracy(model, test_loader, device=rank)\n",
    "        print(f\"[GPU{rank}] Test accuracy\", test_acc)\n",
    "\n",
    "    ####################################################\n",
    "    # NEW:\n",
    "    except ZeroDivisionError as e:\n",
    "        raise ZeroDivisionError(\n",
    "            f\"{e}\\n\\nThis script is designed for 2 GPUs. You can run it as:\\n\"\n",
    "            \"torchrun --nproc_per_node=2 DDP-script-torchrun.py\\n\"\n",
    "            f\"Or, to run it on {torch.cuda.device_count()} GPUs, uncomment the code on lines 103 to 107.\"\n",
    "        )\n",
    "    ####################################################\n",
    "\n",
    "    destroy_process_group()  # NEW: cleanly exit distributed mode\n",
    "\n",
    "\n",
    "def compute_accuracy(model, dataloader, device):\n",
    "    model = model.eval()\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "    return (correct / total_examples).item()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # NEW: Use environment variables set by torchrun if available, otherwise default to single-process.\n",
    "    if \"WORLD_SIZE\" in os.environ:\n",
    "        world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    else:\n",
    "        world_size = 1\n",
    "\n",
    "    if \"LOCAL_RANK\" in os.environ:\n",
    "        rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    elif \"RANK\" in os.environ:\n",
    "        rank = int(os.environ[\"RANK\"])\n",
    "    else:\n",
    "        rank = 0\n",
    "\n",
    "    # Only print on rank 0 to avoid duplicate prints from each GPU process\n",
    "    if rank == 0:\n",
    "        print(\"PyTorch version:\", torch.__version__)\n",
    "        print(\"CUDA available:\", torch.cuda.is_available())\n",
    "        print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    num_epochs = 3\n",
    "    main(rank, world_size, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
